# llm_simulator.py
import time

class MockLLM:
    def __init__(self):
        print(" [LLM] Inicializando n煤cleo de generaci贸n simulada...")

    def generate_response(self, user_query, retrieved_context):
        """
        Simula lo que har铆a GPT-4: Leer el contexto y responder la pregunta.
        """
        print("   Thinking... (Simulando latencia de GPU)")
        time.sleep(1) # Un poco de drama para sentir el realismo
        
        # PROMPT ENGINEERING (As铆 se ver铆a el prompt real)
        # ------------------------------------------------
        # System: Eres un asistente 煤til. Usa el contexto para responder.
        # Context: {retrieved_context}
        # User: {user_query}
        # ------------------------------------------------
        
        # GENERACIN (Simulada con l贸gica de plantillas)
        if not retrieved_context:
            return "Lo siento, mis bancos de memoria no encontraron informaci贸n sobre eso."
        
        # Aqu铆 fingimos que la IA "ley贸" y "entendi贸"
        respuesta = (
            f"隆Hola! He analizado tu base de conocimientos.\n"
            f"Basado en el documento que dice: '{retrieved_context}'...\n"
            f"Puedo responder a tu pregunta '{user_query}' confirmando que esa es la informaci贸n relevante."
        )
        
        return respuesta